{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "onbYxrscs0s_"
   },
   "source": [
    "# T4 : SMS Spam Detector\n",
    "\n",
    "Semester 2221, CSEC 520/620, Team 4\\\n",
    "Assignment 1 - SMS Spam Detector\\\n",
    "Due Sep 15, 2022 11:59 PM EST\\\n",
    "Accounts for 12% of total grade.\n",
    "\n",
    "## Description\n",
    "\n",
    "Welcome to Team 4's SMS Spam Detector. This assignment's goal is to examine both k-NN and Naive Bayes classifiers for determining whether an SMS message is spam or not spam. We will provide some performance metrics in our analsysis to hopefully determine which is more appropriate for this type of classification.\n",
    "\n",
    "## Requirements\n",
    "\n",
    "- Python 3\n",
    "- Must have the `SMSSpamCollection` file in the same directory as this file. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E-VU2uMDZQIN"
   },
   "source": [
    "We must import the various modules and libraries that we will depend on during execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R_Ga6_wUwVFU"
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "import itertools\n",
    "import math\n",
    "import os\n",
    "from os.path import exists\n",
    "import random\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Utility Methods"
   ],
   "metadata": {
    "collapsed": false,
    "id": "c2Qhchl0wlHk"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Similarly to the import statements, we utilize the below utility methods across our models/notebook. This includes detagging tokens, calculating and printing metrics, etc. Run the code block in this section to ensure our utility methods are defined."
   ],
   "metadata": {
    "collapsed": false,
    "id": "kWCA4-HSwlHk"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def detag_tokens(tokens):\n",
    "    \"\"\"\n",
    "    Detags a tagged tokens list. Removes the first element of each child list.\n",
    "\n",
    "    :param tokens: Two dimensional list, with the first element of each child list being a tag ham/spam.\n",
    "    :return: Detagged tokens list.\n",
    "    \"\"\"\n",
    "    # Copy of tokens, without the ham/spam tag.\n",
    "    detagged_tokens = copy.deepcopy(tokens)\n",
    "    detagged_tokens = [token[1:] for token in detagged_tokens]\n",
    "\n",
    "    return detagged_tokens"
   ],
   "metadata": {
    "id": "msL8CmZDwlHl"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def separate_tags(tokens):\n",
    "    \"\"\"\n",
    "    Separates the list of tagged tokens based on the tags ham/spam.\n",
    "\n",
    "    :param tokens: Array containing arrays of individual words. The first word in each array must be either \"ham\" or \"spam\".\n",
    "    :return: Dictionary object containing separated \"ham\" and \"spam\" sets.\n",
    "    \"\"\"\n",
    "    tokens = copy.deepcopy(tokens)\n",
    "\n",
    "    separated_set = {\"ham\": list(filter(lambda token: token[0] == \"ham\", tokens)),\n",
    "                     \"spam\": list(filter(lambda token: token[0] == \"spam\", tokens))}\n",
    "    return separated_set"
   ],
   "metadata": {
    "id": "O9RVJSN9wlHm"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def print_metrics(dictionary, is_percentage=True):\n",
    "    \"\"\"\n",
    "    Prints metrics from a dictionary, metric name is the key and metric result is the value.\n",
    "\n",
    "    :param dictionary: Dictionary containing metric name and metric value.\n",
    "    :param is_percentage: Flag that determines if dictionary metric values will be printed as a percentage.\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "\n",
    "    if is_percentage:\n",
    "        for metric in dictionary:\n",
    "            print(f'{metric + \":\":>20} {dictionary[metric]:024.20%}')\n",
    "    else:\n",
    "        for metric in dictionary:\n",
    "            print(f'{metric + \":\":>20} {dictionary[metric]}')"
   ],
   "metadata": {
    "id": "ivu4fRhpwlHm"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def calculate_metrics(tp, fp, tn, fn, print_results=True, print_only_percentages=True, title=None):\n",
    "    \"\"\"\n",
    "    Calculates classification performance metrics. Can also handle printing of metrics.\n",
    "\n",
    "    :param tp: Number of true positive predictions.\n",
    "    :param fp: Number of false positive predictions.\n",
    "    :param tn: Number of true negative predictions.\n",
    "    :param fn: Number of false negative predictions.\n",
    "    :param print_results: Flag that determines whether results will be printed. True by default.\n",
    "    :param print_only_percentages: Flag that determines whether only percentages, and not raw numbers will be printed. True by default.\n",
    "    :param title: Title of classifier the calculated metrics belong to. None by default.\n",
    "    :return: Dictionary of calculated metrics.\n",
    "    \"\"\"\n",
    "    # Calculate metrics\n",
    "    accuracy = (tp + tn) / (tp + fp + tn + fn)\n",
    "    precision = tp / (tp + fp) if (tp + fp) != 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) != 0 else 0\n",
    "    f1_score = (2 * precision * recall) / (precision + recall) if (precision + recall) != 0 else 0\n",
    "\n",
    "    # Bundle metrics into dictionary\n",
    "    base_metrics = {\"True Positive\": tp, \"False Positive\": fp, \"True Negative\": tn, \"False Negative\": fn}\n",
    "    additional_metrics = {\"Accuracy\": accuracy, \"Precision\": precision, \"Recall\": recall, \"F1-Score\": f1_score}\n",
    "    result_metrics = {**base_metrics, **additional_metrics}\n",
    "\n",
    "    if print_results:\n",
    "        # Heading\n",
    "        header = \" Resulting Performance Metrics \"\n",
    "        print(\"-\" * 45)\n",
    "        print(header.center(45, \"-\"))\n",
    "        if title: print(title.center(len(header), \" \").center(45, \"-\"))\n",
    "        print(\"-\" * 45)\n",
    "\n",
    "        # TP, FP, TN, and FN Numbers\n",
    "        if not print_only_percentages:\n",
    "            print_metrics(base_metrics, False)\n",
    "\n",
    "        # Calculate TP, FP, TN, and FN Rates\n",
    "        total_predictions = sum(base_metrics.values())\n",
    "        rates = {}\n",
    "        for metric in base_metrics:\n",
    "            rates[metric + \" Rate\"] = base_metrics[metric] / total_predictions\n",
    "\n",
    "        # Print All Percentages\n",
    "        print_metrics(rates)\n",
    "        print(\"-----\".center(45, \" \"))\n",
    "        print_metrics(additional_metrics)\n",
    "\n",
    "    # Return dictionary of metrics\n",
    "    return result_metrics"
   ],
   "metadata": {
    "id": "DinMXWMHwlHn"
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3DFs8NUD5OBY"
   },
   "source": [
    "## Tokenizing\n",
    "\n",
    "Our model begins by performing tokenization on the dataset. This takes every line of the file and essentially separates and sanitizes each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hd2RlBDksymF"
   },
   "outputs": [],
   "source": [
    "def tokenize(filename, print_info=False):\n",
    "    \"\"\"\n",
    "    Performs sanitization and then tokenization on the given file.\n",
    "\n",
    "    :param filename: The name or path of the file that contains the data to perform tokenization on.\n",
    "    :param print_info: Flag that determines whether information will be printed. False by default.\n",
    "    :return: An array of sanitized tokens derived from the data housed in the given file.\n",
    "    \"\"\"\n",
    "    file = open(filename, 'r')\n",
    "    lines = [line for line in file]\n",
    "\n",
    "    # First, convert special characters into spaces\n",
    "    clean_lines = [re.sub('\\W+', ' ', line) for line in lines]\n",
    "\n",
    "    # Second, separate each word in each line while also ensuring lowercase\n",
    "    tokens = [line.lower().split() for line in clean_lines]\n",
    "\n",
    "    # Print information\n",
    "    if print_info:\n",
    "        print(f'{\"Lines:\":>18} {lines}')\n",
    "        print(f'{\"Cleaned Lines:\":>18} {clean_lines}')\n",
    "        print(f'{\"Tokens:\":>18} {tokens}')\n",
    "\n",
    "    # Return sanitized tokens\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TBpsM-sk5NnJ"
   },
   "outputs": [],
   "source": [
    "def split_dataset(og_list, percent_train=0.8):\n",
    "    \"\"\"\n",
    "    Splits the original dataset into the training and testing sets. Testing set allocation size is derived from the given training set percentage. Token selection is performed randomly.\n",
    "\n",
    "    :param og_list: The original set to split into training and testing sets.\n",
    "    :param percent_train: The percentage, in decimal form, of the original set to allocate towards training data.\n",
    "    :return: Dictionary object containing allocated \"train\" and \"test\" sets.\n",
    "    \"\"\"\n",
    "    # Get the total number of tokens in the original list\n",
    "    og_total = len(og_list)\n",
    "\n",
    "    # Setup and determine training and testing set allocation\n",
    "    percent_test = round(og_total * (1 - percent_train))\n",
    "    train_set = copy.deepcopy(og_list)\n",
    "    test_set = []\n",
    "\n",
    "    # Fill up the testing set's allocated size by randomly choosing a token from the training set and moving it to the testing set\n",
    "    while percent_test > 0:\n",
    "        selected_token = random.choice(train_set)\n",
    "        test_set.append(selected_token)\n",
    "        train_set.remove(selected_token)\n",
    "        percent_test -= 1\n",
    "\n",
    "    return {\"train\": train_set, \"test\": test_set}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Um6y2t3a-kNt"
   },
   "source": [
    "Execute the code block below to perform tokenization and then split the tokens into training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Perform tokenization on the \"SMSSpamCollection\" file\n",
    "tagged_tokens = tokenize('SMSSpamCollection')\n",
    "\n",
    "# Generate dictionary containing \"train\" and \"test\" sets.\n",
    "dataset = split_dataset(tagged_tokens)"
   ],
   "metadata": {
    "id": "CBO_FgoO6P6s"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C1CHlhh8ZbWp"
   },
   "source": [
    "## k Nearest Neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z9-j171qee6B"
   },
   "source": [
    "In General for K-NN The algorithm would probably be:\n",
    "\n",
    "  Compute TF-IDF vector generation\n",
    "\n",
    "  K-NN comparison stuff\n",
    "\n",
    "  Classify\n",
    "\n",
    "  Compare Against cannon for confusion matrix(TPR, TNR, FPR, FNR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UrZnrWdkcBzj"
   },
   "source": [
    "This next part will be my sub-par attempt to implement TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cCKdRlx7WfpT",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "afb7da9f-243e-4da2-92ad-863834cd746e"
   },
   "outputs": [],
   "source": [
    "def find_unique(og_list):\n",
    "    \"\"\"\n",
    "    Flattens and strips all duplicate words in a given array. Then returns a list of only the unique words, while printing out facts.\n",
    "\n",
    "    :param og_list: Array, can be multidimensional, to find all unique words in.\n",
    "    :return: One dimensional array of all unique words found in the given array.\n",
    "    \"\"\"\n",
    "    # Copy of tokens, without the ham/spam tag.\n",
    "    untag_tokens = copy.deepcopy(og_list)\n",
    "    untag_tokens = [token[1:] for token in untag_tokens]\n",
    "\n",
    "    # Flatten array (2D -> 1D) and remove duplicate words.\n",
    "    unique_tokens = list(itertools.chain.from_iterable(untag_tokens))\n",
    "    unique_tokens = [*set(unique_tokens)]\n",
    "\n",
    "    # Calculate Totals\n",
    "    untagged_total = sum([len(token) for token in untag_tokens])\n",
    "    unique_total = len(unique_tokens)\n",
    "    print(f'{\"Total Words:\":>19} {untagged_total}')\n",
    "    print(f'{\"Total Unique Words:\":>19} {unique_total}')\n",
    "\n",
    "    # return unique words and untagged tokens arrays\n",
    "    return untag_tokens, unique_tokens\n",
    "\n",
    "untagged_tokens, unique_words  = find_unique(dataset.get(\"train\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R1ulybJXppC6"
   },
   "source": [
    "### TF-IDF Calculations\n",
    "It has become clear that we need to find a way to include all dimensions in our calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MafX7w8QQdwf",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "438a417e-0887-40ea-b4a1-0fe05763244e"
   },
   "outputs": [],
   "source": [
    "# First, we get a list of the IDFs for all words\n",
    "# wordsdone is to be sure a word doesn't have more than one entry in the list.\n",
    "idfs = []\n",
    "wordsdone = []\n",
    "\n",
    "# The total document count is always the length of the tokens list\n",
    "doccount=len(untagged_tokens)\n",
    "for word in unique_words:\n",
    "  # Determine the number of documents that have the current word in it\n",
    "  docswithword = 0\n",
    "  for token in untagged_tokens:\n",
    "    if word in token:\n",
    "      docswithword = docswithword + 1\n",
    "  if word not in wordsdone:\n",
    "    if docswithword == 0:\n",
    "      thisidf = [word, 0]\n",
    "      idfs.append(thisidf)\n",
    "    else:\n",
    "      # Calc the IDF (document count/# of documents with word in it) and add it to the list.\n",
    "      thisidf = [word, doccount/docswithword]\n",
    "      idfs.append(thisidf)\n",
    "    wordsdone.append(word)\n",
    "print(idfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GXw3kyJ3eult"
   },
   "outputs": [],
   "source": [
    "# Then we calculate the tf for each word, and use it to get the tf-idf for them.\n",
    "# After calculating them, we write the values of each token to a collective file.\n",
    "if not exists(\"tf_calculations.txt\"):\n",
    "  tf_idfs = open(\"tf_calculations.txt\", \"x\")\n",
    "  tf_idfs = open(\"tf_calculations.txt\", \"w\")\n",
    "\n",
    "\n",
    "  for i in range(len(untagged_tokens)):\n",
    "    # Add an array to separate each token from each other within the list.\n",
    "    tokenlist = []\n",
    "    for word in unique_words:\n",
    "      token = untagged_tokens[i]\n",
    "      # Get the number of times word appears in this token, along with the number of words in it total\n",
    "      thiswordcount = 0\n",
    "      totalwords = len(token)\n",
    "      for t in token:\n",
    "        if word == t:\n",
    "          thiswordcount = thiswordcount + 1\n",
    "      if thiswordcount != 0:\n",
    "        # Get the tf (number of times given word appears/total word count in token)\n",
    "        tf = math.log(thiswordcount/totalwords)\n",
    "        for j in range(len(idfs)):\n",
    "          tok = idfs[j]\n",
    "          if word == tok[0]:\n",
    "            # Get the TF-IDF (tf*idf) and write it to the file.\n",
    "            val = tf*tok[1]\n",
    "            #print(str(val))\n",
    "            tf_idfs.write(word + \",\" + str(val) + \" \")\n",
    "      else:\n",
    "        #print(word)\n",
    "        tf_idfs.write(word + \",0 \")\n",
    "    tf_idfs.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wv2JUJ1tHUlo"
   },
   "source": [
    "Testing of KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OyO4AOGcnco-"
   },
   "outputs": [],
   "source": [
    "def idf_values2(sample):\n",
    "    for i in range(len(sample)):\n",
    "        tf_idfs = []\n",
    "        for word in unique_words:\n",
    "            token = sample[i]\n",
    "            #Get the number of times word appears in this token, along with the number\n",
    "            #of words in it total\n",
    "            thiswordcount = 0\n",
    "            totalwords = len(token)\n",
    "            for t in token:\n",
    "              if word == t:\n",
    "                thiswordcount = thiswordcount + 1\n",
    "            if thiswordcount != 0:\n",
    "            #Get the tf (number of times given word appears/total word count in token)\n",
    "              tf = math.log(thiswordcount/totalwords)\n",
    "              for j in range(len(idfs)):\n",
    "                tok = idfs[j]\n",
    "                if word == tok[0]:\n",
    "                #Get the TF-IDF (tf*idf) and add it to the list.\n",
    "                  val = tf*tok[1]\n",
    "                  vallist = [word, val]\n",
    "                  tf_idfs.append(vallist)\n",
    "            else:\n",
    "                vallist = [word, 0]\n",
    "                tf_idfs.append(vallist)\n",
    "    return tf_idfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J9AePmd5rULV"
   },
   "outputs": [],
   "source": [
    "def filebasedknn(unknown, k):\n",
    "    valuesfile = open(\"tf_calculations.txt\", \"r\")\n",
    "    neighbors=[]\n",
    "    #At the start of the function, a for loop of all tf_idf values are compared \n",
    "    #against the unknown\n",
    "    linenum = 0\n",
    "    #print(\"running knn\")\n",
    "    for line in valuesfile:\n",
    "        #print(\"line \" + str(linenum))\n",
    "                \n",
    "        distance=0\n",
    "        pairs = line.split()\n",
    "        #for every vector in training_data, a euclidean distance is \n",
    "        #calculated against unknown\n",
    "        for i in range(len(pairs)):\n",
    "          currentpair = pairs[i].split(\",\")\n",
    "          if (float(currentpair[1]) == float(0)) and (float(unknown[0][-1]) == float(0)):\n",
    "            continue\n",
    "          #print(i)\n",
    "          if currentpair[0][0] == \"'\":\n",
    "            currentpair[0] = currentpair[0].strip(\"\\'\")\n",
    "          else:\n",
    "            currentpair[0] = currentpair[0].strip('\\\"')\n",
    "          \n",
    "          #print(f'{\"pairs[i]:\":>19} {pairs[i]}')\n",
    "          #print(f'{\"currentpair:\":>19} {currentpair}')\n",
    "          #print(f'{\"i:\":>19} {i}')\n",
    "          #rint(f'{\"currentpair[1]:\":>19} {currentpair[1]}')\n",
    "          #print(f'{\"unknown[0][-1]:\":>19} {unknown[0][-1]}')\n",
    "          #if i == 270:\n",
    "              #print(currentpair)\n",
    "\n",
    "\n",
    "          #The below is the Euclidean distance calculations.\n",
    "          d=math.pow(float(currentpair[1])-float(unknown[0][-1]), 2)          \n",
    "          distance=distance+d\n",
    "        distance=math.pow(distance, .5)\n",
    "        \n",
    "        #if any of the vectors have a distance less than or equal to k,\n",
    "        #it is put in a set of neighbors\n",
    "        if len(neighbors) < k:\n",
    "            neighbors.append([linenum, distance])\n",
    "        else:\n",
    "            for pair in neighbors:\n",
    "              if distance < pair[1]:\n",
    "                neighbors.remove(pair)\n",
    "                neighbors.append([linenum, distance])\n",
    "                break\n",
    "        linenum = linenum + 1\n",
    "\n",
    "    #The identity of neighbours are checked and the identity with the most\n",
    "    #neighbours is the classifier of the unknown\n",
    "    #print(k)\n",
    "    spamC=0\n",
    "    hamC=0\n",
    "    for nn in neighbors:\n",
    "      indexnum = nn[0]\n",
    "      entry = tagged_tokens[indexnum][0]\n",
    "      if entry == \"spam\":\n",
    "          spamC=spamC+1\n",
    "      elif entry == \"ham\":\n",
    "          hamC=hamC+1\n",
    "    if hamC > spamC:\n",
    "        return \"ham\"\n",
    "    elif spamC > hamC:\n",
    "        return \"spam\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fkFkJ8YEDGG9"
   },
   "outputs": [],
   "source": [
    "def tknn(tSet):\n",
    "    # Numbers of each to calculate rate later\n",
    "    tp, fp, tn, fn = 0, 0, 0 , 0\n",
    "\n",
    "    guess = \"\"\n",
    "\n",
    "    msgCount = 0\n",
    "    tcount = 0\n",
    "    while tcount < 2:\n",
    "        unknown = idf_values2(tSet[tcount])\n",
    "        guess = filebasedknn(unknown, 5)\n",
    "        msgCount = msgCount + 1\n",
    "\n",
    "        if tSet[tcount][0] == 'ham':\n",
    "            if guess == 'ham':\n",
    "                tn += 1\n",
    "            elif guess == 'spam':\n",
    "                fp += 1\n",
    "            elif tSet[tcount][0] == 'spam':\n",
    "                if guess == 'ham':\n",
    "                    fn += 1\n",
    "                elif guess == 'spam':\n",
    "                    tp += 1\n",
    "\n",
    "        tcount = tcount + 1\n",
    "\n",
    "    calculate_metrics(tp, fp, tn, fn, title=\"K-Nearest Neighbors\")\n",
    "\n",
    "\n",
    "tknn(dataset.get('test'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HIVsk2tA36ij"
   },
   "source": [
    "## Naive Bayes Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iKK5PTDHjgNO"
   },
   "source": [
    "This function is used to format the idfs of the sample we are trying to identify so that it matches the formatting of the entries in the tf-idf file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9E_U__ot35ei"
   },
   "outputs": [],
   "source": [
    "def naive_bayes_training(ham, spam):\n",
    "    \"\"\"\n",
    "    Performs preliminary naive bayes calculations on the ham and spam set.\n",
    "\n",
    "    :param ham: Array containing the ham set.\n",
    "    :param spam: Array containing the spam set.\n",
    "    :return: Percentages and calculates that will be used by the naive bayes classifier.\n",
    "    \"\"\"\n",
    "    hamWordCounts = {}\n",
    "    spamWordCounts = {}\n",
    "    hamTotal = 0\n",
    "    spamTotal = 0\n",
    "\n",
    "    for msg in ham:\n",
    "        for word in msg:\n",
    "            hamTotal += 1\n",
    "            if word in hamWordCounts:\n",
    "                hamWordCounts[word] = hamWordCounts[word] + 1\n",
    "            else:\n",
    "                hamWordCounts[word] = 1\n",
    "\n",
    "    for msg in spam:\n",
    "        for word in msg:\n",
    "            spamTotal += 1\n",
    "            if word in spamWordCounts:\n",
    "                spamWordCounts[word] = spamWordCounts[word] + 1\n",
    "            else:\n",
    "                spamWordCounts[word] = 1\n",
    "\n",
    "    addNum = 0\n",
    "    for word in hamWordCounts:\n",
    "        if word not in spamWordCounts:\n",
    "            if addNum != 1:\n",
    "                addNum = 1\n",
    "                hamTotal *= 2\n",
    "                spamTotal *= 2\n",
    "            spamWordCounts[word] = 0\n",
    "\n",
    "    for word in spamWordCounts:\n",
    "        if word not in hamWordCounts:\n",
    "            hamWordCounts[word] = 0\n",
    "            if addNum != 1:\n",
    "                addNum = 1\n",
    "                hamTotal *= 2\n",
    "                spamTotal *= 2\n",
    "\n",
    "    hamWPerc = {}\n",
    "    for key in hamWordCounts:\n",
    "        hamWordCounts[key] = hamWordCounts[key] + addNum\n",
    "        hamWPerc[key] = hamWordCounts[key] / hamTotal\n",
    "\n",
    "    spamWPerc = {}\n",
    "    for key in spamWordCounts:\n",
    "        spamWordCounts[key] = spamWordCounts[key] + addNum\n",
    "        spamWPerc[key] = spamWordCounts[key] / spamTotal\n",
    "\n",
    "    initHam = hamTotal / (hamTotal + spamTotal)\n",
    "    initSpam = spamTotal / (hamTotal + spamTotal)\n",
    "\n",
    "    return hamWPerc, spamWPerc, initHam, initSpam\n",
    "\n",
    "\n",
    "separated_tokens = separate_tags(tagged_tokens)\n",
    "hamWPerc, spamWPerc, initHam, initSpam = naive_bayes_training(separated_tokens.get(\"ham\"), separated_tokens.get(\"spam\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wJyxSLYtBkd3"
   },
   "source": [
    "### Naive Bayes Classifier\n",
    "\n",
    "This function performs the classificaiton on new unseen data. Essentially an \"intelligent guessing\" machine using our previosuly calculated metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7tT9_2thBkGd"
   },
   "outputs": [],
   "source": [
    "def naive_bayes_classifier(msg):\n",
    "    \"\"\"\n",
    "    Performs naive bayes classification to determine whether a provided message is spam or ham.\n",
    "\n",
    "    :param msg: A message to perform classification on.\n",
    "    :return: The classification result \"ham\" or \"spam\".\n",
    "    \"\"\"\n",
    "    ham_prob = math.log(initHam)\n",
    "    spam_prob = math.log(initSpam)\n",
    "\n",
    "    for word in msg:\n",
    "        if word != msg[0] and (word != \"ham\" or word != \"spam\"):\n",
    "            if word in hamWPerc:\n",
    "                ham_prob += math.log(hamWPerc[word])\n",
    "\n",
    "    for word in msg:\n",
    "        if word != msg[0] and (word != \"ham\" or word != \"spam\"):\n",
    "            if word in spamWPerc:\n",
    "                spam_prob += math.log(spamWPerc[word])\n",
    "\n",
    "    if ham_prob > spam_prob:\n",
    "        return \"ham\"\n",
    "    elif spam_prob > ham_prob:\n",
    "        return \"spam\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0ZblQZAv_W-2"
   },
   "source": [
    "Testing phase of Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0mn22sin_WtR",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "ecaa1b7d-5757-4941-eff3-bf37e0906dde"
   },
   "outputs": [],
   "source": [
    "def naive_bayes_testing(test_set):\n",
    "    \"\"\"\n",
    "    Performs naive bayes classification on the test set, and compares the results against their actual values.\n",
    "\n",
    "    :param test_set: Test set that the model hasn't been trained on.\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    # Values to calculate rates later\n",
    "    tpn = 0\n",
    "    tnn = 0\n",
    "    fpn = 0\n",
    "    fnn = 0\n",
    "\n",
    "    for msg in test_set:\n",
    "        guess = naive_bayes_classifier(msg)\n",
    "        if msg[0] == 'ham':\n",
    "            if guess == 'ham':\n",
    "                tnn += 1\n",
    "            elif guess == 'spam':\n",
    "                fpn += 1\n",
    "        elif msg[0] == 'spam':\n",
    "            if guess == 'ham':\n",
    "                fnn += 1\n",
    "            elif guess == 'spam':\n",
    "                tpn += 1\n",
    "\n",
    "    calculate_metrics(tpn, fpn, tnn, fnn, title=\"Naive Bayes\")\n",
    "\n",
    "naive_bayes_testing(dataset.get('test'))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
