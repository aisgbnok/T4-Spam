{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "onbYxrscs0s_"
   },
   "source": [
    "# T4 : SMS Spam Detector\n",
    "\n",
    "Semester 2221, CSEC 520/620, Team 4\\\n",
    "Assignment 1 - SMS Spam Detector\\\n",
    "Due Sep 15, 2022 11:59 PM EST\\\n",
    "Accounts for 12% of total grade.\n",
    "\n",
    "> **ANTHONY REWRITE**\\\n",
    "> Don't submit this one.\n",
    "\n",
    "## Description\n",
    "\n",
    "Welcome to Team 4's SMS Spam Detector. This assignment's goal is to examine both k-NN and Naive Bayes classifiers for determining whether an SMS message is spam or not spam. We will provide some performance metrics in our analysis to hopefully determine which is more appropriate for this type of classification.\n",
    "\n",
    "## Requirements\n",
    "\n",
    "- Python 3\n",
    "- Must have the `SMSSpamCollection` file in the same directory as this file. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E-VU2uMDZQIN"
   },
   "source": [
    "To start, we must import the various modules and libraries that we will depend on during execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R_Ga6_wUwVFU"
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "from collections import Counter\n",
    "import itertools\n",
    "import math\n",
    "import random\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Utility Methods"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Similarly to the import statements, we utilize the below utility methods across our models/notebook. This includes detagging tokens, calculating and printing metrics, etc. Run the code block in this section to ensure our utility methods are defined."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def detag_tokens(tokens):\n",
    "    \"\"\"\n",
    "    Detags a tagged tokens list. Removes the first element of each child list.\n",
    "\n",
    "    :param tokens: Two dimensional list, with the first element of each child list being a tag ham/spam.\n",
    "    :return: Detagged tokens list.\n",
    "    \"\"\"\n",
    "    # Copy of tokens, without the ham/spam tag.\n",
    "    detagged_tokens = copy.deepcopy(tokens)\n",
    "    detagged_tokens = [token[1:] for token in detagged_tokens]\n",
    "\n",
    "    return detagged_tokens"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def separate_tags(tokens):\n",
    "    \"\"\"\n",
    "    Separates the list of tagged tokens based on the tags ham/spam.\n",
    "\n",
    "    :param tokens: Array containing arrays of individual words. The first word in each array must be either \"ham\" or \"spam\".\n",
    "    :return: Dictionary object containing separated \"ham\" and \"spam\" sets.\n",
    "    \"\"\"\n",
    "    tokens = copy.deepcopy(tokens)\n",
    "\n",
    "    separated_set = {\"ham\": list(filter(lambda token: token[0] == \"ham\", tokens)),\n",
    "                     \"spam\": list(filter(lambda token: token[0] == \"spam\", tokens))}\n",
    "    return separated_set"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def print_metrics(dictionary, is_percentage=True):\n",
    "    \"\"\"\n",
    "    Prints metrics from a dictionary, metric name is the key and metric result is the value.\n",
    "\n",
    "    :param dictionary: Dictionary containing metric name and metric value.\n",
    "    :param is_percentage: Flag that determines if dictionary metric values will be printed as a percentage.\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "\n",
    "    if is_percentage:\n",
    "        for metric in dictionary:\n",
    "            print(f'{metric + \":\":>20} {dictionary[metric]:024.20%}')\n",
    "    else:\n",
    "        for metric in dictionary:\n",
    "            print(f'{metric + \":\":>20} {dictionary[metric]}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def calculate_metrics(tp, fp, tn, fn, print_results=True, print_only_percentages=True, title=None):\n",
    "    \"\"\"\n",
    "    Calculates classification performance metrics. Can also handle printing of metrics.\n",
    "\n",
    "    :param tp: Number of true positive predictions.\n",
    "    :param fp: Number of false positive predictions.\n",
    "    :param tn: Number of true negative predictions.\n",
    "    :param fn: Number of false negative predictions.\n",
    "    :param print_results: Flag that determines whether results will be printed. True by default.\n",
    "    :param print_only_percentages: Flag that determines whether only percentages, and not raw numbers will be printed. True by default.\n",
    "    :param title: Title of classifier the calculated metrics belong to. None by default.\n",
    "    :return: Dictionary of calculated metrics.\n",
    "    \"\"\"\n",
    "    # Calculate metrics\n",
    "    accuracy = (tp + tn) / (tp + fp + tn + fn)\n",
    "    precision = tp / (tp + fp) if (tp + fp) != 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) != 0 else 0\n",
    "    f1_score = (2 * precision * recall) / (precision + recall) if (precision + recall) != 0 else 0\n",
    "\n",
    "    # Bundle metrics into dictionary\n",
    "    base_metrics = {\"True Positive\": tp, \"False Positive\": fp, \"True Negative\": tn, \"False Negative\": fn}\n",
    "    additional_metrics = {\"Accuracy\": accuracy, \"Precision\": precision, \"Recall\": recall, \"F1-Score\": f1_score}\n",
    "    result_metrics = {**base_metrics, **additional_metrics}\n",
    "\n",
    "    if print_results:\n",
    "        # Heading\n",
    "        header = \" Resulting Performance Metrics \"\n",
    "        print(\"-\" * 45)\n",
    "        print(header.center(45, \"-\"))\n",
    "        if title: print(title.center(len(header), \" \").center(45, \"-\"))\n",
    "        print(\"-\" * 45)\n",
    "\n",
    "        # TP, FP, TN, and FN Numbers\n",
    "        if not print_only_percentages:\n",
    "            print_metrics(base_metrics, False)\n",
    "\n",
    "        # Calculate TP, FP, TN, and FN Rates\n",
    "        total_predictions = sum(base_metrics.values())\n",
    "        rates = {}\n",
    "        for metric in base_metrics:\n",
    "            rates[metric + \" Rate\"] = base_metrics[metric] / total_predictions\n",
    "\n",
    "        # Print All Percentages\n",
    "        print_metrics(rates)\n",
    "        print(\"-----\".center(45, \" \"))\n",
    "        print_metrics(additional_metrics)\n",
    "\n",
    "    # Return dictionary of metrics\n",
    "    return result_metrics"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Tokenizing\n",
    "\n",
    "Our model begins by performing tokenization on the dataset. This takes every line of the file and essentially separates and sanitizes each word."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def tokenize(filename, print_info=False):\n",
    "    \"\"\"\n",
    "    Performs sanitization and then tokenization on the given file.\n",
    "\n",
    "    :param filename: The name or path of the file that contains the data to perform tokenization on.\n",
    "    :param print_info: Flag that determines whether information will be printed. False by default.\n",
    "    :return: An array of sanitized tokens derived from the data housed in the given file.\n",
    "    \"\"\"\n",
    "    file = open(filename, 'r')\n",
    "    lines = [line for line in file]\n",
    "\n",
    "    # First, convert special characters into spaces\n",
    "    clean_lines = [re.sub('\\W+', ' ', line) for line in lines]\n",
    "\n",
    "    # Second, separate each word in each line while also ensuring lowercase\n",
    "    tokens = [line.lower().split() for line in clean_lines]\n",
    "\n",
    "    # Print information\n",
    "    if print_info:\n",
    "        print(f'{\"Lines:\":>18} {lines}')\n",
    "        print(f'{\"Cleaned Lines:\":>18} {clean_lines}')\n",
    "        print(f'{\"Tokens:\":>18} {tokens}')\n",
    "\n",
    "    # Return sanitized tokens\n",
    "    return tokens"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def split_dataset(og_list, percent_train=0.8):\n",
    "    \"\"\"\n",
    "    Splits the original dataset into the training and testing sets. Testing set allocation size is derived from the given training set percentage. Token selection is performed randomly.\n",
    "\n",
    "    :param og_list: The original set to split into training and testing sets.\n",
    "    :param percent_train: The percentage, in decimal form, of the original set to allocate towards training data.\n",
    "    :return: Dictionary object containing allocated \"train\" and \"test\" sets.\n",
    "    \"\"\"\n",
    "    # Get the total number of tokens in the original list\n",
    "    og_total = len(og_list)\n",
    "\n",
    "    # Setup and determine training and testing set allocation\n",
    "    percent_test = round(og_total * (1 - percent_train))\n",
    "    train_set = copy.deepcopy(og_list)\n",
    "    test_set = []\n",
    "\n",
    "    # Fill up the testing set's allocated size by randomly choosing a token from the training set and moving it to the testing set\n",
    "    while percent_test > 0:\n",
    "        selected_token = random.choice(train_set)\n",
    "        test_set.append(selected_token)\n",
    "        train_set.remove(selected_token)\n",
    "        percent_test -= 1\n",
    "\n",
    "    return {\"train\": train_set, \"test\": test_set}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Execute the code block below to perform tokenization and then split the tokens into training and testing sets."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Perform tokenization on the \"SMSSpamCollection\" file\n",
    "tagged_tokens = tokenize('SMSSpamCollection')\n",
    "\n",
    "# Generate dictionary containing \"train\" and \"test\" sets.\n",
    "dataset = split_dataset(tagged_tokens)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now that our data has been successfully tokenized and split, we can move on to classification."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## k Nearest Neighbors"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### TF-IDF Methods"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def calculate_idf(corpus):\n",
    "    \"\"\"\n",
    "    Calculate the inverse document frequency of each term across all documents. This measures the importance of the term.\n",
    "\n",
    "    :param corpus: A list of documents.\n",
    "    :return: Dictionary containing a unique term as the key, and its idf as the value.\n",
    "    \"\"\"\n",
    "    # Flatten list (2D -> 1D); list of terms\n",
    "    flat_corpus = list(itertools.chain.from_iterable(corpus))\n",
    "\n",
    "    # Count how many times a term occurs across corpus, while also removing duplicates\n",
    "    counted_terms = Counter(flat_corpus)\n",
    "\n",
    "    # Calculate Inverse Document Frequency\n",
    "    document_count = len(corpus)\n",
    "    idf_equation = lambda terms, term: math.log(document_count / terms[term])\n",
    "    # idf_equation = lambda terms, term: math.log((document_count + 1) / (terms[term] + 1)) + 1 # Consider adding 1 to each?\n",
    "    idf = {term: idf_equation(counted_terms, term) for term in counted_terms}\n",
    "\n",
    "    # Return the IDFs\n",
    "    return idf"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def calculate_tf(document):\n",
    "    \"\"\"\n",
    "    Calculate the term frequency of each term inside the document. This measures how frequently a term occurs in a document.\n",
    "\n",
    "    :param document: A document containing terms.\n",
    "    :return: A dictionary generated from the provided document, key being the term and value being the term frequency.\n",
    "    \"\"\"\n",
    "\n",
    "    # Count how many times a term occurs in the document\n",
    "    counted_document = Counter(document)\n",
    "\n",
    "    # Calculate Term Frequency\n",
    "    tf_equation = lambda doc, term: doc[term] / sum(doc.values())\n",
    "    tf = {term: tf_equation(counted_document, term) for term in counted_document}\n",
    "\n",
    "    # Return the TFs\n",
    "    return tf"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def calculate_tf_idf(document, idf_values):\n",
    "\n",
    "    # Calculate TF-IDF\n",
    "    # For each term in the document, take the term's TF and multiply by the term's IDF.\n",
    "    get_idf = lambda term: idf_values.get(term) if idf_values.get(term) is not None else 0\n",
    "    return {term: document[term] * get_idf(term) for term in document}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### K-NN Training and Classification"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def knn_training(corpus):\n",
    "    # Remove the ham/spam tags from the corpus.\n",
    "    corpus = detag_tokens(corpus)\n",
    "\n",
    "    # Calculate the IDF of each word across the corpus\n",
    "    idf_values = calculate_idf(corpus)\n",
    "\n",
    "    # Calculate the TFs for each document across the corpus\n",
    "    tf_values = [calculate_tf(document) for document in corpus]\n",
    "\n",
    "    # Calculate TF-IDF for each term in each document\n",
    "    tf_idf_values = [calculate_tf_idf(document, idf_values) for document in tf_values]\n",
    "\n",
    "    return idf_values, tf_idf_values\n",
    "\n",
    "\n",
    "test = [['ham', 'how', 'is', 'your', 'day', 'going', 'thomas'],\n",
    "        ['ham', 'i', 'am', 'happy', 'that', 'things', 'are', 'well'],\n",
    "        ['spam', 'give', 'me', 'all', 'of', 'your', 'money', 'now']]\n",
    "corpus_idf, corpus_tf_idf = knn_training(test)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def calculate_ed(q, p):\n",
    "    print(\"q - Train Tf-IDF\", q)\n",
    "    print(\"p - New TF-IDF\", p)\n",
    "\n",
    "    # Some garbage implementation because I'm too tired to make this better atm\n",
    "    dictnew = {}\n",
    "\n",
    "    for value in q:\n",
    "        if p.get(value) is not None:\n",
    "            dictnew[value] = q.get(value)\n",
    "\n",
    "    for value in p:\n",
    "        if q.get(value) is not None:\n",
    "            if dictnew.get(value) is not None:\n",
    "                dictnew[value] = (dictnew.get(value) - q.get(value)) ** 2\n",
    "\n",
    "    print(dictnew)\n",
    "    distance = sum(dictnew.values())\n",
    "\n",
    "    # distance = 0\n",
    "    # for i in range(len(q)):\n",
    "    #     distance += (q[i] - p[i]) ** 2\n",
    "\n",
    "    return math.sqrt(distance)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def knn_classifier(message, idf_values, tf_idf_values):\n",
    "\n",
    "    # Calculate the TF for the message\n",
    "    msg_tf = calculate_tf(message)\n",
    "    # print(msg_tf)\n",
    "\n",
    "    # Calculate TF-IDF for the message\n",
    "    msg_tf_idf = calculate_tf_idf(msg_tf, idf_values)\n",
    "    # print(msg_tf_idf)\n",
    "\n",
    "    print(calculate_ed(tf_idf_values[0], msg_tf_idf))\n",
    "\n",
    "    # Compute euclidean between each entry in the tf_idf_values and the msg_tf_idf\n",
    "    # for value in tf_idf_values:\n",
    "    #     calculate_ed(value, message)\n",
    "\n",
    "new_msg = ['i', 'want', 'all', 'of', 'your', 'money']\n",
    "knn_classifier(new_msg, corpus_idf, corpus_tf_idf)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Naive Bayes Classification"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def naive_bayes_training(ham, spam):\n",
    "    \"\"\"\n",
    "    Performs preliminary naive bayes calculations on the ham and spam set.\n",
    "\n",
    "    :param ham: Array containing the ham set.\n",
    "    :param spam: Array containing the spam set.\n",
    "    :return: Percentages and calculates that will be used by the naive bayes classifier.\n",
    "    \"\"\"\n",
    "    hamWordCounts = {}\n",
    "    spamWordCounts = {}\n",
    "    hamTotal = 0\n",
    "    spamTotal = 0\n",
    "\n",
    "    for msg in ham:\n",
    "        for word in msg:\n",
    "            hamTotal += 1\n",
    "            if word in hamWordCounts:\n",
    "                hamWordCounts[word] = hamWordCounts[word] + 1\n",
    "            else:\n",
    "                hamWordCounts[word] = 1\n",
    "\n",
    "    for msg in spam:\n",
    "        for word in msg:\n",
    "            spamTotal += 1\n",
    "            if word in spamWordCounts:\n",
    "                spamWordCounts[word] = spamWordCounts[word] + 1\n",
    "            else:\n",
    "                spamWordCounts[word] = 1\n",
    "\n",
    "    addNum = 0\n",
    "    for word in hamWordCounts:\n",
    "        if word not in spamWordCounts:\n",
    "            if addNum != 1:\n",
    "                addNum = 1\n",
    "                hamTotal *= 2\n",
    "                spamTotal *= 2\n",
    "            spamWordCounts[word] = 0\n",
    "\n",
    "    for word in spamWordCounts:\n",
    "        if word not in hamWordCounts:\n",
    "            hamWordCounts[word] = 0\n",
    "            if addNum != 1:\n",
    "                addNum = 1\n",
    "                hamTotal *= 2\n",
    "                spamTotal *= 2\n",
    "\n",
    "    hamWPerc = {}\n",
    "    for key in hamWordCounts:\n",
    "        hamWordCounts[key] = hamWordCounts[key] + addNum\n",
    "        hamWPerc[key] = hamWordCounts[key] / hamTotal\n",
    "\n",
    "    spamWPerc = {}\n",
    "    for key in spamWordCounts:\n",
    "        spamWordCounts[key] = spamWordCounts[key] + addNum\n",
    "        spamWPerc[key] = spamWordCounts[key] / spamTotal\n",
    "\n",
    "    initHam = hamTotal / (hamTotal + spamTotal)\n",
    "    initSpam = spamTotal / (hamTotal + spamTotal)\n",
    "\n",
    "    return hamWPerc, spamWPerc, initHam, initSpam\n",
    "\n",
    "\n",
    "separated_tokens = separate_tags(tagged_tokens)\n",
    "hamWPerc, spamWPerc, initHam, initSpam = naive_bayes_training(separated_tokens.get(\"ham\"), separated_tokens.get(\"spam\"))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Naive Bayes Classifier\n",
    "\n",
    "This function performs the classificaiton on new unseen data. Essentially an \"intelligent guessing\" machine using our previosuly calculated metrics."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def naive_bayes_classifier(msg):\n",
    "    \"\"\"\n",
    "    Performs naive bayes classification to determine whether a provided message is spam or ham.\n",
    "\n",
    "    :param msg: A message to perform classification on.\n",
    "    :return: The classification result \"ham\" or \"spam\".\n",
    "    \"\"\"\n",
    "    ham_prob = math.log(initHam)\n",
    "    spam_prob = math.log(initSpam)\n",
    "\n",
    "    for word in msg:\n",
    "        if word != msg[0] and (word != \"ham\" or word != \"spam\"):\n",
    "            if word in hamWPerc:\n",
    "                ham_prob += math.log(hamWPerc[word])\n",
    "\n",
    "    for word in msg:\n",
    "        if word != msg[0] and (word != \"ham\" or word != \"spam\"):\n",
    "            if word in spamWPerc:\n",
    "                spam_prob += math.log(spamWPerc[word])\n",
    "\n",
    "    if ham_prob > spam_prob:\n",
    "        return \"ham\"\n",
    "    elif spam_prob > ham_prob:\n",
    "        return \"spam\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def naive_bayes_testing(test_set):\n",
    "    \"\"\"\n",
    "    Performs naive bayes classification on the test set, and compares the results against their actual values.\n",
    "\n",
    "    :param test_set: Test set that the model hasn't been trained on.\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    # Values to calculate rates later\n",
    "    tpn = 0\n",
    "    tnn = 0\n",
    "    fpn = 0\n",
    "    fnn = 0\n",
    "\n",
    "    for msg in test_set:\n",
    "        guess = naive_bayes_classifier(msg)\n",
    "        if msg[0] == 'ham':\n",
    "            if guess == 'ham':\n",
    "                tnn += 1\n",
    "            elif guess == 'spam':\n",
    "                fpn += 1\n",
    "        elif msg[0] == 'spam':\n",
    "            if guess == 'ham':\n",
    "                fnn += 1\n",
    "            elif guess == 'spam':\n",
    "                tpn += 1\n",
    "\n",
    "    calculate_metrics(tpn, fpn, tnn, fnn, title=\"Naive Bayes\")\n",
    "\n",
    "naive_bayes_testing(dataset.get('test'))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Once we are finished executing it is best practice to flush our changes and unmount our personal Google Drive."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "drive.flush_and_unmount()"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
